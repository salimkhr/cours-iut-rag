Ce projet met en Å“uvre un moteur RAG (Retrieval-Augmented Generation) en Python, permettant de rÃ©pondre Ã  des questions en s'appuyant sur des documents de cours vectorisÃ©s (HTML, CSS, React, etc.).

ğŸ“‚ Structure du projet :
------------------------

- `app.py`               : Script principal pour lancer une requÃªte ou interagir avec le systÃ¨me
- `rag_system.py`        : Moteur RAG basÃ© sur embeddings + FAISS
- `text_processor.py`    : DÃ©coupage et nettoyage des blocs de texte
- `dataloader.py`        : Chargement des fichiers `.json` contenant les contenus de cours
- `rag/corpus/`          : Contient les fichiers de cours (format JSON)
- `venv/`                : Environnement virtuel Python (non versionnÃ©)
- `rag/index/`           : (optionnel) dossier pour stocker un index persistÃ©

ğŸ› ï¸ FonctionnalitÃ©s principales :
-------------------------------

âœ” Chargement automatique des fichiers de cours JSON  
âœ” Nettoyage, dÃ©coupage en chunks, dÃ©duplication  
âœ” Embedding avec SentenceTransformers (`all-MiniLM-L6-v2`)  
âœ” Recherche vectorielle rapide avec FAISS  
âœ” Construction dâ€™un prompt clair pour lâ€™LLM  
âœ” Reload automatique en cas de modification du corpus  
âœ” Compatible avec une interface externe (ex: Next.js)

âš™ï¸ Lancer le projet :
---------------------

1. CrÃ©e ton environnement virtuel (si ce nâ€™est pas dÃ©jÃ  fait) : `python -m venv venv`
2. Active-le : sous Windows : `venv\Scripts\activate`, sous linux `source venv/bin/activate`
3. Installe les dÃ©pendances : `pip install -r requirements.txt`
4. Place tes fichiers `.json` dans `rag/corpus/`.  
   Format attendu :
```json
{
  "title": "HTML & CSS",
  "file": "RappelHtml.tsx",
  "blocks": [
    { "type": "cours", "content": "Voici un rappel sur les balises HTML..." },
    ...
  ]
}
```
5. Lance lâ€™application : `python app.py`
6. Entrez votre question : Comment fonctionne le modÃ¨le de boÃ®te en CSS ?
Le moteur cherchera les extraits les plus pertinents dans le corpus vectorisÃ©, construira un prompt, et pourra lâ€™envoyer Ã  un LLM (Ollama, OpenAI, etc.).

ğŸ”— Connexion avec une interface Next.js :
-------------------------------
Ce backend peut Ãªtre utilisÃ© via API (FastAPI, Flask...) pour alimenter une interface React/Next.js.

Le frontend envoie la question â†’ le backend retourne la rÃ©ponse gÃ©nÃ©rÃ©e et les sources citÃ©es.

ğŸ“¦ DÃ©pendances principales :
-------------------------------
sentence-transformers

faiss-cpu

numpy

tqdm

ğŸ§  Fonctionnement rÃ©sumÃ© :
-------------------------------
Lecture des fichiers JSON (cours)

Nettoyage & dÃ©coupage en chunks

Embedding des chunks

Indexation FAISS

Recherche vectorielle sur question

Construction du prompt

Utilisation du LLM pour gÃ©nÃ©rer une rÃ©ponse

ğŸ”„ Reload intelligent :
-------------------------------
Le systÃ¨me dÃ©tecte automatiquement les changements dans rag/corpus et reconstruit lâ€™index au besoin. Un rechargement forcÃ© est aussi possible via : `rag.force_reload()`
