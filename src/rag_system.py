import os
import time
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataloader import CourseDataLoader
from text_processor import TextProcessor


class VectorRAG:
    """
    Repr√©sente un syst√®me de recherche dense utilisant une approche Retrieval-Augmented Generation (RAG).
    Il permet d‚Äôassocier les requ√™tes des utilisateurs √† des documents pertinents
    dans un corpus pr√©-index√©, en s‚Äôappuyant sur des embeddings et des mesures de similarit√©
    pour une r√©cup√©ration efficace de l‚Äôinformation.

    Cette classe int√®gre des fonctions pour g√©rer l‚Äôindexation des documents,
    retrouver des documents similaires selon les requ√™tes, et calculer la similarit√© cosinus.
    Elle utilise un mod√®le SentenceTransformer pour g√©n√©rer les embeddings, FAISS pour l‚Äôindex vectoriel,
    et inclut des m√©canismes de gestion et mise √† jour du corpus.

    :ivar model_name: Nom du mod√®le transformer utilis√© pour g√©n√©rer les embeddings.
    :type model_name: str
    :ivar corpus_dir: Chemin du dossier contenant les fichiers du corpus √† indexer.
    :type corpus_dir: str
    :ivar similarity_metric: M√©trique utilis√©e pour le calcul de similarit√©. Par d√©faut "cosine".
    :type similarity_metric: str
    :ivar model: Instance du mod√®le SentenceTransformer pour la g√©n√©ration d‚Äôembeddings.
    :type model: Optional[SentenceTransformer]
    :ivar index: Index FAISS utilis√© pour stocker et rechercher les vecteurs d‚Äôembeddings.
    :type index: Optional[faiss.Index]
    :ivar meta: M√©tadonn√©es associ√©es aux documents index√©s pour faciliter la r√©cup√©ration.
    :type meta: Optional[List[Dict]]
    :ivar last_modified: Timestamp de la derni√®re modification du corpus.
    :type last_modified: float
    :ivar data_loader: Instance responsable du chargement des donn√©es du cours depuis le disque.
    :type data_loader: CourseDataLoader
    :ivar text_processor: Composant qui traite et pr√©pare les textes pour la g√©n√©ration d‚Äôembeddings.
    :type text_processor: TextProcessor
    """


    def __init__(self, model_name: str, corpus_dir: str, similarity_metric: str = "cosine"):
        self.model_name = model_name
        self.corpus_dir = corpus_dir
        self.similarity_metric = similarity_metric  # "cosine" ou "l2"
        self.model = None
        self.index = None
        self.meta = None
        self.last_modified = 0

        # Initialisation des composants
        self.data_loader = CourseDataLoader(corpus_dir)
        self.text_processor = TextProcessor()

        print(f"üîÑ Initialisation du syst√®me RAG avec {similarity_metric} similarity...")
        self._load_model()
        self._check_and_reload_if_needed()

    def _load_model(self):
        """
        Charge un mod√®le SentenceTransformer s‚Äôil n‚Äôest pas d√©j√† charg√©.

        Cette m√©thode v√©rifie si le mod√®le est actuellement charg√©.
        Si le mod√®le n‚Äôest pas charg√© (c‚Äôest-√†-dire s‚Äôil est `None`),
        un mod√®le SentenceTransformer correspondant √† l‚Äôattribut `model_name`
        est charg√© et assign√© √† l‚Äôattribut `model`.

        :raises RuntimeError: En cas de probl√®me lors du chargement du mod√®le.

        :return: None
        """
        if self.model is None:
            print(f"üîÑ Chargement du mod√®le {self.model_name}...")
            self.model = SentenceTransformer(self.model_name)
            print("‚úÖ Mod√®le charg√©")

    def _get_corpus_last_modified(self) -> float:
        """
        D√©termine la date de la derni√®re modification parmi tous les fichiers markdown (.md)
        dans un r√©pertoire donn√© ainsi que ses sous-r√©pertoires.
        Si le r√©pertoire n‚Äôexiste pas, retourne 0.

        :param self: L‚Äôinstance de la classe contenant l‚Äôattribut du r√©pertoire du corpus.
        :return: Le timestamp (en secondes depuis l‚Äô√©poque) du fichier markdown le plus r√©cemment modifi√©,
                 ou 0 si le r√©pertoire n‚Äôexiste pas ou ne contient aucun fichier markdown.
        :rtype: float
        """
        if not os.path.exists(self.corpus_dir):
            return 0

        latest_time = 0
        for root, dirs, files in os.walk(self.corpus_dir):
            for file in files:
                if file.endswith('.md'):
                    file_path = os.path.join(root, file)
                    file_time = os.path.getmtime(file_path)
                    latest_time = max(latest_time, file_time)
        return latest_time

    def _normalize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:
        """
       Normalise les embeddings pour en faire des vecteurs unitaires.

        Cette m√©thode prend un tableau NumPy 2D d‚Äôembeddings et normalise chaque vecteur
        pour qu‚Äôil ait une norme √©gale √† 1. Cela garantit que tous les embeddings se trouvent
        sur l‚Äôhypersph√®re unit√©, ce qui est souvent n√©cessaire pour les calculs de similarit√©.

        :param embeddings: Tableau NumPy 2D de forme (n_samples, n_features), o√π chaque ligne repr√©sente un vecteur embedding.
        :return: Tableau NumPy 2D de forme (n_samples, n_features), o√π chaque ligne est un vecteur unitaire normalis√© correspondant √† la ligne d‚Äôentr√©e.
        """
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        return embeddings / norms

    def _build_index(self):
        """
        Construit l‚Äôindex des embeddings textuels √† partir des blocs de cours charg√©s.

        Cette fonction pr√©pare et encode les donn√©es textuelles pour la recherche.
        Elle traite le texte, g√©n√®re les embeddings avec un mod√®le pr√©d√©fini,
        et les stocke dans un index FAISS adapt√© selon la m√©trique de similarit√© choisie.
        Les embeddings peuvent √™tre configur√©s pour utiliser la similarit√© cosinus ou la distance euclidienne.
        Si aucun bloc de cours ou texte valide n‚Äôest trouv√©, l‚Äôindex et les m√©tadonn√©es sont r√©initialis√©s.

        :raises Exception: En cas d‚Äôerreur lors du traitement des donn√©es, de la g√©n√©ration des embeddings ou de la cr√©ation de l‚Äôindex.
        """
        try:
            course_blocks = self.data_loader.load_all_course_blocks()
            if not course_blocks:
                print("‚ö†Ô∏è Aucun fichier de cours trouv√©")
                self.index = None
                self.meta = []
                return

            texts, meta = self.text_processor.prepare_texts_for_embedding(course_blocks)
            if not texts:
                print("‚ö†Ô∏è Aucun contenu √† indexer")
                self.index = None
                self.meta = []
                return

            print(f"üîÑ G√©n√©ration des embeddings pour {len(texts)} chunks...")
            embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)

            dimension = embeddings.shape[1]

            # Choix de l'index selon la m√©trique
            if self.similarity_metric == "cosine":
                # Pour cosine similarity, on normalise les embeddings
                embeddings = self._normalize_embeddings(embeddings)
                # IndexFlatIP calcule le dot product (√©quivalent √† cosine sur vecteurs normalis√©s)
                self.index = faiss.IndexFlatIP(dimension)
                print("üìê Utilisation de Cosine Similarity (IndexFlatIP)")
            else:
                # Distance L2 classique
                self.index = faiss.IndexFlatL2(dimension)
                print("üìè Utilisation de L2 Distance")

            self.index.add(embeddings.astype('float32'))
            self.meta = meta

            print(f"‚úÖ Index cr√©√© avec {len(texts)} documents")

        except Exception as e:
            print(f"‚ùå Erreur lors de la construction de l'index: {e}")
            self.index = None
            self.meta = []

    def _check_and_reload_if_needed(self) -> bool:
        """
        V√©rifie et recharge les donn√©es si n√©cessaire, en fonction du timestamp de modification ou
        si le rechargement est forc√©. Cette m√©thode contr√¥le la date de derni√®re modification du corpus
        et d√©cide de reconstruire l‚Äôindex selon les changements d√©tect√©s ou si un rechargement est demand√©.

        :return: Bool√©en indiquant si les donn√©es ont √©t√© recharg√©es ou non.
        """
        current_modified = self._get_corpus_last_modified()

        if current_modified > self.last_modified:
            print("üîÑ Rechargement des donn√©es du corpus...")
            self.last_modified = current_modified
            self._build_index()
            return True
        else:
            print("‚ùå Rechargement non necessaire")
        return False

    def search_similar(self, query: str, k: int = 5, include_neighbors: bool = True) -> List[Dict]:
        """
        Recherche des √©l√©ments similaires dans les donn√©es index√©es √† partir de la requ√™te fournie,
        et retourne une liste des correspondances avec leurs scores de similarit√©.
        Cette fonction utilise le mod√®le pr√©-entra√Æn√© pour g√©n√©rer les embeddings,
        et recherche dans l‚Äôindex les r√©sultats les plus pertinents selon la m√©trique de similarit√©.
        Le r√©sultat peut optionnellement inclure les √©l√©ments voisins adjacents aux entr√©es correspondantes dans les m√©tadonn√©es.

        :param query: La requ√™te textuelle √† rechercher. Elle est encod√©e en embedding avant la comparaison de similarit√©.
        :type query: str
        :param k: Nombre de meilleurs r√©sultats √† retourner. Par d√©faut 5.
        :type k: int
        :param include_neighbors: Indique si les √©l√©ments voisins doivent √™tre inclus dans les r√©sultats. Par d√©faut True.
        :type include_neighbors: bool
        :return: Une liste de dictionnaires repr√©sentant les r√©sultats de la recherche.
                 Chaque dictionnaire contient les m√©tadonn√©es de l‚Äô√©l√©ment trouv√© et son score de similarit√© calcul√©.
                 Les voisins, s‚Äôils sont inclus, ont le m√™me score de similarit√©.
        :rtype: List[Dict]
        """
        self._check_and_reload_if_needed()

        if self.index is None or not self.meta:
            return []

        try:
            query_emb = self.model.encode([query], convert_to_numpy=True)

            # Normaliser la requ√™te si on utilise cosine similarity
            if self.similarity_metric == "cosine":
                query_emb = self._normalize_embeddings(query_emb)

            scores, indices = self.index.search(query_emb.astype('float32'), k)

            seen_indices = set()
            results = []

            for i, score in zip(indices[0], scores[0]):
                if i >= len(self.meta):
                    continue

                # Ajouter chunk principal
                if i not in seen_indices:
                    result = self.meta[i].copy()

                    # Conversion du score selon la m√©trique
                    if self.similarity_metric == "cosine":
                        # IndexFlatIP retourne le dot product (plus √©lev√© = plus similaire)
                        result['similarity_score'] = float(score)
                        result['cosine_similarity'] = float(score)  # Score d√©j√† normalis√©
                    else:
                        # IndexFlatL2 retourne la distance (plus bas = plus similaire)
                        result['similarity_score'] = float(score)
                        result['l2_distance'] = float(score)

                    results.append(result)
                    seen_indices.add(i)

                # Ajouter les voisins
                if include_neighbors:
                    for neighbor_offset in [-1, 1]:
                        ni = i + neighbor_offset
                        if 0 <= ni < len(self.meta) and ni not in seen_indices:
                            neighbor = self.meta[ni].copy()
                            neighbor['similarity_score'] = float(score)
                            results.append(neighbor)
                            seen_indices.add(ni)

            # Trier par score (d√©croissant pour cosine, croissant pour L2)
            if self.similarity_metric == "cosine":
                results.sort(key=lambda x: x['similarity_score'], reverse=True)
            else:
                results.sort(key=lambda x: x['similarity_score'])

            return results

        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche: {e}")
            return []

    def calculate_cosine_similarity_manual(self, text1: str, text2: str) -> float:
        """
        Calcule la similarit√© cosinus entre deux textes d‚Äôentr√©e en utilisant le mod√®le fourni,
        sans utiliser de fonctions pr√©construites de similarit√© cosinus.

        La similarit√© cosinus est calcul√©e manuellement √† partir du produit scalaire des vecteurs
        et de leurs magnitudes respectives.

        :param text1: Premier texte d‚Äôentr√©e pour le calcul.
        :type text1: str
        :param text2: Second texte d‚Äôentr√©e pour le calcul.
        :type text2: str
        :return: Le score de similarit√© cosinus entre les deux textes en nombre flottant.
        :rtype: float
        """
        emb1 = self.model.encode([text1], convert_to_numpy=True)
        emb2 = self.model.encode([text2], convert_to_numpy=True)

        # Calcul manuel du cosine
        dot_product = np.dot(emb1[0], emb2[0])
        norm1 = np.linalg.norm(emb1[0])
        norm2 = np.linalg.norm(emb2[0])

        return float(dot_product / (norm1 * norm2))

    def build_rag_prompt(self, results: List[Dict], question: str) -> str:
        """
        Construit un prompt pour r√©pondre √† une question en utilisant les r√©sultats fournis.
        Cette fonction g√©n√®re un contexte structur√© √† partir des r√©sultats, qui sert de base √† la r√©ponse,
        tout en fournissant des instructions d√©taill√©es sur la mani√®re dont la r√©ponse doit √™tre formul√©e.

        :param results: Liste de dictionnaires repr√©sentant des extraits pertinents de cours universitaires.
                        Chaque dictionnaire contient des attributs tels que 'content', 'file', 'type',
                        et √©ventuellement 'cosine_similarity' ou 'l2_distance' pour l‚Äôinformation de pertinence.
        :type results: List[Dict]
        :param question: La question √† laquelle il faut r√©pondre en s‚Äôappuyant sur les r√©sultats et le contexte fournis.
        :type question: str
        :return: Un prompt structur√© et d√©taill√© utilis√© pour g√©n√©rer une r√©ponse claire et p√©dagogique.
        :rtype: str
        """
        if not results:
            return f"""R√©ponds √† la question suivante du mieux que tu peux :\n\nQuestion : {question}\n\nR√©ponse :"""

        context = ""
        for i, r in enumerate(results, 1):
            score_info = ""
            if 'cosine_similarity' in r:
                score_info = f" (similarit√©: {r['cosine_similarity']:.3f})"
            elif 'l2_distance' in r:
                score_info = f" (distance: {r['l2_distance']:.3f})"

            context += f"\n### Extrait {i} ({r['file']}) ‚Äì {r['type']}{score_info}\n{r['content']}\n"

        prompt = f"""Contexte :

{context}

Instructions :
- R√©ponds √† la question
- Structure ta r√©ponse de mani√®re concise et p√©dagogique
- Utilise uniquement les informations contenues dans le contexte
- Cite les sources quand c'est pertinent

Question : {question}

R√©ponse :"""

        return prompt.strip()

    def get_stats(self) -> Dict:
        """
        Fournit des m√©tadonn√©es statistiques sur l‚Äô√©tat actuel de l‚Äôobjet.

        :return: Un dictionnaire contenant les m√©tadonn√©es suivantes :
                 - model_name : Le nom du mod√®le utilis√©.
                 - corpus_dir : Le chemin du r√©pertoire du corpus.
                 - similarity_metric : L‚Äôidentifiant de la m√©trique de similarit√© utilis√©e.
                 - indexed_documents : Le nombre de documents index√©s.
                 - last_reload : Une cha√Æne de caract√®res repr√©sentant le timestamp de la derni√®re modification.
                 - index_ready : Un bool√©en indiquant si l‚Äôindex est pr√™t (initialis√©).
        :rtype: Dict
        """
        return {
            "model_name": self.model_name,
            "corpus_dir": self.corpus_dir,
            "similarity_metric": self.similarity_metric,
            "indexed_documents": len(self.meta) if self.meta else 0,
            "last_reload": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.last_modified)),
            "index_ready": self.index is not None
        }

    def force_reload(self):
        """
        Force le rechargement d‚Äôune ressource ou d‚Äôune configuration,
        en contournant toutes les v√©rifications conditionnelles qui emp√™cheraient normalement ce rechargement.
        Elle d√©finit l‚Äôattribut ``last_modified`` √† 0 et d√©clenche un rechargement forc√©
        en appelant le m√©canisme de rechargement.

        :raises RuntimeError: En cas d‚Äôerreur inattendue lors de l‚Äôex√©cution du rechargement.
        """
        print("üîÑ Rechargement forc√©...")
        self.last_modified = 0
        self._check_and_reload_if_needed()